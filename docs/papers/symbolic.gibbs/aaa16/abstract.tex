\begin{abstract}
\begin{quote}
In real-world applications of probabilistic reasoning, instead of random variables (non)linear functions of them (i.e.\ constraints over them) may be observed. 
In case an observed constraint is a reversible function, the model can be handled via reducing the 
dimensionality of parameter space by Jacobian-based change of variable.
Using the properties of the Dirac delta, our first contribution is to propose a more general mechanism that does not require reversibility.   
Nevertheless, dimension reduction can lead to highly piecewise densities over which the performance of the existing inference tools can be unsatisfactory.   
Our second contribution is to present a variation of Gibbs sampling that efficiently handles piecewise densities and therefore works on constraint models in a purview that have remained beyond the tractability of the existing methods.
The key insight is to introduce a class of functions that (1)~is sufficiently rich to approximate arbitrary models up to arbitrary precision, (2)~is closed under dimension reduction for models with (non)linear algebraic constraints and (3)~always permits one analytical integral --- sufficient to \emph{automatically} derive conditionals for Gibbs sampling in closed form. 
Our experimental results show that the proposed sampler 
converges at least an order of magnitude faster than existing Monte Carlo
samplers.
\end{quote}
\end{abstract}